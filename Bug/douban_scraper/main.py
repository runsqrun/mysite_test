#!/usr/bin/env python3
"""
è±†ç“£ç”µå½±è¯„è®ºçˆ¬è™« - ä¸»ç¨‹åº
çˆ¬å–è±†ç“£ç”µå½±çš„æ‰€æœ‰è¯„è®ºï¼Œå¹¶è¿›è¡Œåˆ†ç±»ç»Ÿè®¡

ç›®æ ‡ç”µå½±: https://movie.douban.com/subject/36176155/

ä½¿ç”¨æ–¹æ³•:
    1. é¦–æ¬¡è¿è¡Œéœ€è¦ç™»å½•: python main.py --login
    2. çˆ¬å–æ‰€æœ‰æ•°æ®: python main.py --scrape
    3. åªåˆ†æå·²æœ‰æ•°æ®: python main.py --analyze
    4. å®Œæ•´æµç¨‹: python main.py --all
"""
import argparse
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, PROJECT_ROOT)

from src.scraper import DoubanScraper
from src.classifier import CommentClassifier
from config.settings import DATA_DIR, MOVIE_URL


def print_banner():
    """æ‰“å°æ¬¢è¿ä¿¡æ¯"""
    banner = """
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                          â•‘
    â•‘          ğŸ¬ è±†ç“£ç”µå½±è¯„è®ºçˆ¬è™« & åˆ†æå·¥å…· ğŸ¬               â•‘
    â•‘                                                          â•‘
    â•‘   åŠŸèƒ½: çˆ¬å–ç”µå½±è¯„è®º | æƒ…æ„Ÿåˆ†æ | è¯„åˆ†ç»Ÿè®¡ | åˆ†ç±»æ•´ç†    â•‘
    â•‘                                                          â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    print(banner)
    print(f"    ç›®æ ‡ç”µå½±: {MOVIE_URL}\n")


def login():
    """æ‰‹åŠ¨ç™»å½•è±†ç“£"""
    print("\nğŸ“ å¯åŠ¨ç™»å½•æ¨¡å¼...")
    print("=" * 50)
    
    scraper = DoubanScraper(headless=False)
    try:
        scraper.login_manual()
        print("\nâœ… ç™»å½•æˆåŠŸï¼Cookieå·²ä¿å­˜ï¼Œä¸‹æ¬¡è¿è¡Œå°†è‡ªåŠ¨ä½¿ç”¨ã€‚")
    except Exception as e:
        print(f"\nâŒ ç™»å½•å¤±è´¥: {e}")
    finally:
        scraper.stop()


def scrape(max_comment_pages: int = None, max_review_pages: int = None):
    """
    çˆ¬å–è¯„è®ºæ•°æ®
    
    Args:
        max_comment_pages: çŸ­è¯„æœ€å¤§é¡µæ•°
        max_review_pages: é•¿è¯„æœ€å¤§é¡µæ•°
    """
    print("\nğŸ•·ï¸ å¯åŠ¨çˆ¬è™«æ¨¡å¼...")
    print("=" * 50)
    
    scraper = DoubanScraper(headless=False)  # é¦–æ¬¡å»ºè®®æ˜¾ç¤ºæµè§ˆå™¨
    
    try:
        # çˆ¬å–æ‰€æœ‰æ•°æ®
        data = scraper.scrape_all(
            max_comment_pages=max_comment_pages,
            max_review_pages=max_review_pages
        )
        
        # ä¿å­˜åŸå§‹æ•°æ®
        scraper.save_raw_data()
        
        print("\nâœ… çˆ¬å–å®Œæˆï¼")
        print(f"   çŸ­è¯„: {len(data.get('comments', []))} æ¡")
        print(f"   é•¿è¯„: {len(data.get('reviews', []))} æ¡")
        
        return data
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜å·²çˆ¬å–çš„æ•°æ®...")
        scraper.save_raw_data()
        print("æ•°æ®å·²ä¿å­˜ã€‚")
    except Exception as e:
        print(f"\nâŒ çˆ¬å–å‡ºé”™: {e}")
        scraper.save_raw_data()
        raise
    finally:
        scraper.stop()


def analyze():
    """åˆ†æå·²çˆ¬å–çš„æ•°æ®"""
    print("\nğŸ“Š å¯åŠ¨åˆ†ææ¨¡å¼...")
    print("=" * 50)
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    comments_file = os.path.join(DATA_DIR, 'comments.csv')
    reviews_file = os.path.join(DATA_DIR, 'reviews.csv')
    
    if not os.path.exists(comments_file):
        print(f"âŒ æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {comments_file}")
        print("è¯·å…ˆè¿è¡Œçˆ¬è™«: python main.py --scrape")
        return
    
    # åˆ›å»ºåˆ†ç±»å™¨å¹¶åŠ è½½æ•°æ®
    classifier = CommentClassifier()
    classifier.load_from_csv(comments_file, reviews_file)
    
    # æ‰§è¡Œåˆ†ç±»
    classifier.classify_all()
    
    # ç”Ÿæˆç»Ÿè®¡
    classifier.generate_statistics()
    
    # æ‰“å°æ‘˜è¦
    classifier.print_summary()
    
    # ä¿å­˜ç»“æœ
    classifier.save_results()
    
    print("\nâœ… åˆ†æå®Œæˆï¼")


def run_all(max_comment_pages: int = None, max_review_pages: int = None):
    """è¿è¡Œå®Œæ•´æµç¨‹ï¼šçˆ¬å– + åˆ†æ"""
    print("\nğŸš€ å¯åŠ¨å®Œæ•´æµç¨‹...")
    
    # çˆ¬å–æ•°æ®
    data = scrape(max_comment_pages, max_review_pages)
    
    if data and data.get('comments'):
        # ç›´æ¥ä½¿ç”¨çˆ¬å–çš„æ•°æ®è¿›è¡Œåˆ†æ
        classifier = CommentClassifier(
            comments=data.get('comments', []),
            reviews=data.get('reviews', [])
        )
        
        # æ‰§è¡Œåˆ†ç±»
        classifier.classify_all()
        
        # ç”Ÿæˆç»Ÿè®¡
        classifier.generate_statistics()
        
        # æ‰“å°æ‘˜è¦
        classifier.print_summary()
        
        # ä¿å­˜ç»“æœ
        classifier.save_results()
        
        print("\nâœ… å®Œæ•´æµç¨‹æ‰§è¡Œå®Œæ¯•ï¼")
    else:
        print("\nâš ï¸ æ²¡æœ‰çˆ¬å–åˆ°æ•°æ®ï¼Œè·³è¿‡åˆ†ææ­¥éª¤ã€‚")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='è±†ç“£ç”µå½±è¯„è®ºçˆ¬è™« & åˆ†æå·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python main.py --login                    # é¦–æ¬¡è¿è¡Œï¼Œæ‰‹åŠ¨ç™»å½•
  python main.py --scrape                   # çˆ¬å–æ‰€æœ‰è¯„è®º
  python main.py --scrape --pages 5         # åªçˆ¬å–å‰5é¡µçŸ­è¯„
  python main.py --analyze                  # åˆ†æå·²æœ‰æ•°æ®
  python main.py --all                      # çˆ¬å– + åˆ†æ
  python main.py --all --pages 10           # çˆ¬å–å‰10é¡µ + åˆ†æ
        """
    )
    
    parser.add_argument('--login', action='store_true',
                        help='æ‰‹åŠ¨ç™»å½•è±†ç“£è´¦å·ï¼ˆé¦–æ¬¡è¿è¡Œéœ€è¦ï¼‰')
    parser.add_argument('--scrape', action='store_true',
                        help='çˆ¬å–è¯„è®ºæ•°æ®')
    parser.add_argument('--analyze', action='store_true',
                        help='åˆ†æå·²çˆ¬å–çš„æ•°æ®')
    parser.add_argument('--all', action='store_true',
                        help='è¿è¡Œå®Œæ•´æµç¨‹ï¼ˆçˆ¬å–+åˆ†æï¼‰')
    parser.add_argument('--pages', type=int, default=None,
                        help='æœ€å¤§çˆ¬å–é¡µæ•°ï¼ˆé»˜è®¤çˆ¬å–å…¨éƒ¨ï¼‰')
    parser.add_argument('--review-pages', type=int, default=None,
                        help='é•¿è¯„æœ€å¤§çˆ¬å–é¡µæ•°ï¼ˆé»˜è®¤åŒ--pagesï¼‰')
    
    args = parser.parse_args()
    
    # æ‰“å°æ¬¢è¿ä¿¡æ¯
    print_banner()
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šä»»ä½•æ“ä½œï¼Œæ˜¾ç¤ºå¸®åŠ©
    if not any([args.login, args.scrape, args.analyze, args.all]):
        parser.print_help()
        print("\nğŸ’¡ å¿«é€Ÿå¼€å§‹:")
        print("   1. é¦–æ¬¡è¿è¡Œ: python main.py --login")
        print("   2. çˆ¬å–æ•°æ®: python main.py --scrape")
        print("   3. åˆ†ææ•°æ®: python main.py --analyze")
        return
    
    # æ‰§è¡Œå¯¹åº”æ“ä½œ
    if args.login:
        login()
    
    if args.scrape:
        review_pages = args.review_pages or args.pages
        scrape(max_comment_pages=args.pages, max_review_pages=review_pages)
    
    if args.analyze:
        analyze()
    
    if args.all:
        review_pages = args.review_pages or args.pages
        run_all(max_comment_pages=args.pages, max_review_pages=review_pages)


if __name__ == '__main__':
    main()
