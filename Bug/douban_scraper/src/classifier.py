"""
è¯„è®ºåˆ†ç±»å™¨ - å¯¹è±†ç“£ç”µå½±è¯„è®ºè¿›è¡Œåˆ†ç±»å’Œç»Ÿè®¡
æ”¯æŒæŒ‰è¯„åˆ†ã€æƒ…æ„Ÿã€çƒ­åº¦ç­‰å¤šç§æ–¹å¼åˆ†ç±»
"""
import json
import os
from typing import List, Dict, Tuple
from collections import Counter
import pandas as pd

try:
    from snownlp import SnowNLP
except ImportError:
    print("è¯·å®‰è£…snownlp: pip install snownlp")
    SnowNLP = None

import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.settings import (
    RATING_TEXT_MAP, RATING_CATEGORIES,
    SENTIMENT_POSITIVE_THRESHOLD, SENTIMENT_NEGATIVE_THRESHOLD,
    DATA_DIR, OUTPUT_STATS_JSON, OUTPUT_CLASSIFIED_JSON
)


class CommentClassifier:
    """è¯„è®ºåˆ†ç±»å™¨"""
    
    def __init__(self, comments: List[Dict] = None, reviews: List[Dict] = None):
        """
        åˆå§‹åŒ–åˆ†ç±»å™¨
        
        Args:
            comments: çŸ­è¯„åˆ—è¡¨
            reviews: é•¿è¯„åˆ—è¡¨
        """
        self.comments = comments or []
        self.reviews = reviews or []
        self.classified_data = {}
        self.statistics = {}
    
    def load_from_csv(self, comments_file: str = None, reviews_file: str = None):
        """
        ä»CSVæ–‡ä»¶åŠ è½½æ•°æ®
        
        Args:
            comments_file: çŸ­è¯„CSVæ–‡ä»¶è·¯å¾„
            reviews_file: é•¿è¯„CSVæ–‡ä»¶è·¯å¾„
        """
        if comments_file and os.path.exists(comments_file):
            df = pd.read_csv(comments_file)
            self.comments = df.to_dict('records')
            print(f"å·²åŠ è½½ {len(self.comments)} æ¡çŸ­è¯„")
        
        if reviews_file and os.path.exists(reviews_file):
            df = pd.read_csv(reviews_file)
            self.reviews = df.to_dict('records')
            print(f"å·²åŠ è½½ {len(self.reviews)} æ¡é•¿è¯„")
    
    def analyze_sentiment(self, text: str) -> Tuple[float, str]:
        """
        åˆ†ææ–‡æœ¬æƒ…æ„Ÿ
        
        Args:
            text: å¾…åˆ†æçš„æ–‡æœ¬
            
        Returns:
            (æƒ…æ„Ÿåˆ†æ•°0-1, æƒ…æ„Ÿç±»åˆ«)
        """
        if not text or not SnowNLP:
            return 0.5, "ä¸­æ€§"
        
        try:
            s = SnowNLP(text)
            score = s.sentiments
            
            if score >= SENTIMENT_POSITIVE_THRESHOLD:
                sentiment = "æ­£é¢"
            elif score <= SENTIMENT_NEGATIVE_THRESHOLD:
                sentiment = "è´Ÿé¢"
            else:
                sentiment = "ä¸­æ€§"
            
            return score, sentiment
            
        except Exception as e:
            print(f"æƒ…æ„Ÿåˆ†æå‡ºé”™: {e}")
            return 0.5, "ä¸­æ€§"
    
    def classify_by_rating(self) -> Dict[str, List[Dict]]:
        """
        æŒ‰è¯„åˆ†åˆ†ç±»
        
        Returns:
            æŒ‰è¯„åˆ†åˆ†ç±»çš„è¯„è®ºå­—å…¸
        """
        result = {category: [] for category in RATING_CATEGORIES.keys()}
        
        for comment in self.comments:
            rating = comment.get('rating', 0)
            for category, ratings in RATING_CATEGORIES.items():
                if rating in ratings:
                    result[category].append(comment)
                    break
        
        self.classified_data['by_rating'] = result
        return result
    
    def classify_by_sentiment(self) -> Dict[str, List[Dict]]:
        """
        æŒ‰æƒ…æ„Ÿåˆ†ç±»
        
        Returns:
            æŒ‰æƒ…æ„Ÿåˆ†ç±»çš„è¯„è®ºå­—å…¸
        """
        result = {
            "æ­£é¢": [],
            "ä¸­æ€§": [],
            "è´Ÿé¢": []
        }
        
        print("æ­£åœ¨è¿›è¡Œæƒ…æ„Ÿåˆ†æ...")
        for i, comment in enumerate(self.comments):
            content = comment.get('content', '')
            score, sentiment = self.analyze_sentiment(content)
            
            # æ·»åŠ æƒ…æ„Ÿä¿¡æ¯åˆ°è¯„è®º
            comment['sentiment_score'] = score
            comment['sentiment'] = sentiment
            
            result[sentiment].append(comment)
            
            # è¿›åº¦æ˜¾ç¤º
            if (i + 1) % 100 == 0:
                print(f"å·²åˆ†æ {i + 1}/{len(self.comments)} æ¡è¯„è®º")
        
        self.classified_data['by_sentiment'] = result
        return result
    
    def classify_by_popularity(self, top_n: int = 100) -> Dict[str, List[Dict]]:
        """
        æŒ‰çƒ­åº¦ï¼ˆæœ‰ç”¨æ•°ï¼‰åˆ†ç±»
        
        Args:
            top_n: çƒ­é—¨è¯„è®ºæ•°é‡
            
        Returns:
            çƒ­åº¦åˆ†ç±»ç»“æœ
        """
        sorted_comments = sorted(
            self.comments,
            key=lambda x: x.get('votes', 0),
            reverse=True
        )
        
        result = {
            "çƒ­é—¨è¯„è®º": sorted_comments[:top_n],
            "æ™®é€šè¯„è®º": sorted_comments[top_n:]
        }
        
        self.classified_data['by_popularity'] = result
        return result
    
    def classify_all(self) -> Dict:
        """
        æ‰§è¡Œæ‰€æœ‰åˆ†ç±»
        
        Returns:
            æ‰€æœ‰åˆ†ç±»ç»“æœ
        """
        print("\n" + "="*50)
        print("å¼€å§‹åˆ†ç±»è¯„è®º...")
        print("="*50)
        
        print("\n1. æŒ‰è¯„åˆ†åˆ†ç±»...")
        self.classify_by_rating()
        
        print("\n2. æŒ‰æƒ…æ„Ÿåˆ†ç±»...")
        self.classify_by_sentiment()
        
        print("\n3. æŒ‰çƒ­åº¦åˆ†ç±»...")
        self.classify_by_popularity()
        
        return self.classified_data
    
    def generate_statistics(self) -> Dict:
        """
        ç”Ÿæˆç»Ÿè®¡æ•°æ®
        
        Returns:
            ç»Ÿè®¡ç»“æœå­—å…¸
        """
        stats = {
            "æ€»è¯„è®ºæ•°": {
                "çŸ­è¯„": len(self.comments),
                "é•¿è¯„": len(self.reviews),
                "åˆè®¡": len(self.comments) + len(self.reviews)
            },
            "è¯„åˆ†åˆ†å¸ƒ": {},
            "æƒ…æ„Ÿåˆ†å¸ƒ": {},
            "çƒ­åº¦ç»Ÿè®¡": {},
            "å…³é”®è¯ç»Ÿè®¡": {}
        }
        
        # è¯„åˆ†åˆ†å¸ƒç»Ÿè®¡
        if 'by_rating' in self.classified_data:
            for category, comments in self.classified_data['by_rating'].items():
                count = len(comments)
                percentage = count / len(self.comments) * 100 if self.comments else 0
                stats["è¯„åˆ†åˆ†å¸ƒ"][category] = {
                    "æ•°é‡": count,
                    "å æ¯”": f"{percentage:.1f}%"
                }
        
        # è¯¦ç»†è¯„åˆ†ç»Ÿè®¡ï¼ˆ1-5æ˜Ÿï¼‰
        rating_counter = Counter([c.get('rating', 0) for c in self.comments])
        stats["è¯¦ç»†è¯„åˆ†"] = {}
        for rating in range(5, 0, -1):
            count = rating_counter.get(rating, 0)
            percentage = count / len(self.comments) * 100 if self.comments else 0
            stats["è¯¦ç»†è¯„åˆ†"][f"{rating}æ˜Ÿ ({RATING_TEXT_MAP.get(rating, '')})"] = {
                "æ•°é‡": count,
                "å æ¯”": f"{percentage:.1f}%"
            }
        
        # æƒ…æ„Ÿåˆ†å¸ƒç»Ÿè®¡
        if 'by_sentiment' in self.classified_data:
            for sentiment, comments in self.classified_data['by_sentiment'].items():
                count = len(comments)
                percentage = count / len(self.comments) * 100 if self.comments else 0
                stats["æƒ…æ„Ÿåˆ†å¸ƒ"][sentiment] = {
                    "æ•°é‡": count,
                    "å æ¯”": f"{percentage:.1f}%"
                }
        
        # çƒ­åº¦ç»Ÿè®¡
        if self.comments:
            votes = [c.get('votes', 0) for c in self.comments]
            stats["çƒ­åº¦ç»Ÿè®¡"] = {
                "æœ€é«˜æœ‰ç”¨æ•°": max(votes) if votes else 0,
                "å¹³å‡æœ‰ç”¨æ•°": f"{sum(votes) / len(votes):.1f}" if votes else 0,
                "æœ‰ç”¨æ•°>100çš„è¯„è®º": len([v for v in votes if v > 100]),
                "æœ‰ç”¨æ•°>1000çš„è¯„è®º": len([v for v in votes if v > 1000])
            }
        
        # é«˜é¢‘è¯ç»Ÿè®¡
        stats["å…³é”®è¯ç»Ÿè®¡"] = self._extract_keywords()
        
        self.statistics = stats
        return stats
    
    def _extract_keywords(self, top_n: int = 20) -> Dict[str, int]:
        """
        æå–é«˜é¢‘å…³é”®è¯
        
        Args:
            top_n: è¿”å›çš„å…³é”®è¯æ•°é‡
            
        Returns:
            å…³é”®è¯åŠå…¶é¢‘æ¬¡
        """
        if not SnowNLP:
            return {}
        
        all_text = ' '.join([c.get('content', '') for c in self.comments])
        
        if not all_text:
            return {}
        
        try:
            s = SnowNLP(all_text)
            # è·å–å…³é”®è¯
            keywords = s.keywords(top_n)
            
            # ç»Ÿè®¡è¯é¢‘
            word_counter = Counter()
            for comment in self.comments:
                content = comment.get('content', '')
                for keyword in keywords:
                    if keyword in content:
                        word_counter[keyword] += content.count(keyword)
            
            return dict(word_counter.most_common(top_n))
            
        except Exception as e:
            print(f"å…³é”®è¯æå–å‡ºé”™: {e}")
            return {}
    
    def get_sample_comments(self, category: str, n: int = 5) -> List[Dict]:
        """
        è·å–å„åˆ†ç±»çš„ç¤ºä¾‹è¯„è®º
        
        Args:
            category: åˆ†ç±»åç§°
            n: ç¤ºä¾‹æ•°é‡
            
        Returns:
            ç¤ºä¾‹è¯„è®ºåˆ—è¡¨
        """
        if 'by_rating' in self.classified_data and category in self.classified_data['by_rating']:
            comments = self.classified_data['by_rating'][category]
            # æŒ‰æœ‰ç”¨æ•°æ’åºï¼Œå–æœ€çƒ­é—¨çš„
            sorted_comments = sorted(comments, key=lambda x: x.get('votes', 0), reverse=True)
            return sorted_comments[:n]
        
        if 'by_sentiment' in self.classified_data and category in self.classified_data['by_sentiment']:
            comments = self.classified_data['by_sentiment'][category]
            sorted_comments = sorted(comments, key=lambda x: x.get('votes', 0), reverse=True)
            return sorted_comments[:n]
        
        return []
    
    def print_summary(self):
        """æ‰“å°åˆ†ç±»æ‘˜è¦"""
        if not self.statistics:
            self.generate_statistics()
        
        print("\n" + "="*60)
        print("ğŸ“Š è¯„è®ºç»Ÿè®¡æ‘˜è¦")
        print("="*60)
        
        # æ€»æ•°
        print(f"\nğŸ“ æ€»è¯„è®ºæ•°:")
        print(f"   çŸ­è¯„: {self.statistics['æ€»è¯„è®ºæ•°']['çŸ­è¯„']} æ¡")
        print(f"   é•¿è¯„: {self.statistics['æ€»è¯„è®ºæ•°']['é•¿è¯„']} æ¡")
        
        # è¯„åˆ†åˆ†å¸ƒ
        print(f"\nâ­ è¯„åˆ†åˆ†å¸ƒ:")
        if "è¯¦ç»†è¯„åˆ†" in self.statistics:
            for rating, data in self.statistics["è¯¦ç»†è¯„åˆ†"].items():
                bar = "â–ˆ" * int(float(data['å æ¯”'].rstrip('%')) / 5)
                print(f"   {rating}: {data['æ•°é‡']:>5} ({data['å æ¯”']:>5}) {bar}")
        
        # æƒ…æ„Ÿåˆ†å¸ƒ
        print(f"\nğŸ˜Š æƒ…æ„Ÿåˆ†å¸ƒ:")
        if "æƒ…æ„Ÿåˆ†å¸ƒ" in self.statistics:
            emoji_map = {"æ­£é¢": "ğŸ˜Š", "ä¸­æ€§": "ğŸ˜", "è´Ÿé¢": "ğŸ˜¢"}
            for sentiment, data in self.statistics["æƒ…æ„Ÿåˆ†å¸ƒ"].items():
                emoji = emoji_map.get(sentiment, "")
                bar = "â–ˆ" * int(float(data['å æ¯”'].rstrip('%')) / 5)
                print(f"   {emoji} {sentiment}: {data['æ•°é‡']:>5} ({data['å æ¯”']:>5}) {bar}")
        
        # çƒ­é—¨å…³é”®è¯
        print(f"\nğŸ”‘ çƒ­é—¨å…³é”®è¯:")
        if "å…³é”®è¯ç»Ÿè®¡" in self.statistics and self.statistics["å…³é”®è¯ç»Ÿè®¡"]:
            keywords = list(self.statistics["å…³é”®è¯ç»Ÿè®¡"].items())[:10]
            print("   " + ", ".join([f"{k}({v})" for k, v in keywords]))
        
        # ç¤ºä¾‹è¯„è®º
        print(f"\nğŸ“Œ å„åˆ†ç±»çƒ­é—¨è¯„è®ºç¤ºä¾‹:")
        for category in ["å¥½è¯„", "å·®è¯„"]:
            samples = self.get_sample_comments(category, 2)
            if samples:
                print(f"\n   ã€{category}ã€‘")
                for i, sample in enumerate(samples, 1):
                    content = sample.get('content', '')[:50] + "..." if len(sample.get('content', '')) > 50 else sample.get('content', '')
                    print(f"   {i}. {content} (ğŸ‘{sample.get('votes', 0)})")
    
    def save_results(self):
        """ä¿å­˜åˆ†ç±»ç»“æœå’Œç»Ÿè®¡æ•°æ®"""
        os.makedirs(DATA_DIR, exist_ok=True)
        
        # ä¿å­˜ç»Ÿè®¡æ•°æ®
        stats_file = os.path.join(DATA_DIR, OUTPUT_STATS_JSON)
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(self.statistics, f, ensure_ascii=False, indent=2)
        print(f"\nç»Ÿè®¡æ•°æ®å·²ä¿å­˜åˆ°: {stats_file}")
        
        # ä¿å­˜åˆ†ç±»ç»“æœï¼ˆåªä¿å­˜æ‘˜è¦ï¼Œå®Œæ•´æ•°æ®å¤ªå¤§ï¼‰
        classified_summary = {}
        for classify_type, data in self.classified_data.items():
            classified_summary[classify_type] = {}
            for category, comments in data.items():
                classified_summary[classify_type][category] = {
                    "count": len(comments),
                    "samples": [
                        {
                            "content": c.get('content', '')[:100],
                            "rating": c.get('rating', 0),
                            "votes": c.get('votes', 0)
                        }
                        for c in sorted(comments, key=lambda x: x.get('votes', 0), reverse=True)[:5]
                    ]
                }
        
        classified_file = os.path.join(DATA_DIR, OUTPUT_CLASSIFIED_JSON)
        with open(classified_file, 'w', encoding='utf-8') as f:
            json.dump(classified_summary, f, ensure_ascii=False, indent=2)
        print(f"åˆ†ç±»ç»“æœå·²ä¿å­˜åˆ°: {classified_file}")
        
        # ä¿å­˜å¸¦æƒ…æ„Ÿæ ‡æ³¨çš„å®Œæ•´æ•°æ®
        if self.comments:
            df = pd.DataFrame(self.comments)
            sentiment_file = os.path.join(DATA_DIR, 'comments_with_sentiment.csv')
            df.to_csv(sentiment_file, index=False, encoding='utf-8-sig')
            print(f"å¸¦æƒ…æ„Ÿæ ‡æ³¨çš„è¯„è®ºå·²ä¿å­˜åˆ°: {sentiment_file}")


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # æµ‹è¯•åˆ†ç±»å™¨
    classifier = CommentClassifier()
    
    # ä»æ–‡ä»¶åŠ è½½æ•°æ®
    comments_file = os.path.join(DATA_DIR, 'comments.csv')
    reviews_file = os.path.join(DATA_DIR, 'reviews.csv')
    
    if os.path.exists(comments_file):
        classifier.load_from_csv(comments_file, reviews_file)
        classifier.classify_all()
        classifier.generate_statistics()
        classifier.print_summary()
        classifier.save_results()
    else:
        print("è¯·å…ˆè¿è¡Œçˆ¬è™«è·å–æ•°æ®")
